"""
API routes and endpoints for the FastAPI application.
"""

import os
from typing import Any, Optional, List
from fastapi import APIRouter, Request, HTTPException
from openai import AsyncOpenAI, AsyncStream
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from fastapi.responses import StreamingResponse
from .prompt_utils import extract_prompt_content, clean_prompt_content
router = APIRouter()

mini_deployment = os.environ['MINI_DEPLOYMENT']


def get_openai_client(request: Request) -> AsyncOpenAI:
    """
    Get the OpenAI client from the FastAPI app state.
    
    Args:
        request (Request): FastAPI request object
        
    Returns:
        OpenAI: The OpenAI client instance
        
    Raises:
        HTTPException: If the client is not available
    """
    client = getattr(request.app.state, 'openai_client', None)
    if client is None:
        raise HTTPException(
            status_code=500, 
            detail="OpenAI client not initialized"
        )
    return client


@router.post("/v1/chat/completions")
async def create_chat_completion(
    request: Request, 
    chat_request: dict[Any, Any]
) -> StreamingResponse: # | AsyncStream[ChatCompletionChunk]:
    """
    Create a chat completion using OpenAI API.
    """
    try:
        # Get the OpenAI client from app state
        openai_client = get_openai_client(request)
        assert chat_request.get('stream', False) is True, "Only streaming responses are supported currently"

        chat_request['model'] = mini_deployment
        
        prompt = chat_request['messages'][-1]['content']
        
        # Do cleanup pass

        cleanup_prompt = """Given the prompt provided inside the <prompt> tags, improve the prompt and do no other actions like generate code or answer questions.
- Translate any text which is not English into English, whilst retaining the original meaning and keeping technical terms like "Python" or anything in backticks as-is.
- Fix any spelling or grammatical errors.
- If there are contradictory instructions, resolve them in a sensible way.
- If you cannot resolve contradictions, response with a question asking for clarification and the tag <needs_clarification>true</needs_clarification> at the end of your response.
Respond with the original prompt if no improvements are needed.
        """
        cleanup_result = await openai_client.chat.completions.create(
            model=mini_deployment,
            messages=[
                {"role": "system", "content": cleanup_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            # max_tokens=1000,
            stream=False
        )

        print(f"üßπ Cleaned up prompt: {cleanup_result.choices[0].message.content}")

        if '<needs_clarification>true</needs_clarification>' in cleanup_result.choices[0].message.content:
            print("‚ùì Needs clarification, stopping here.")
            async def clarification_generator():
                yield f"data: {cleanup_result.choices[0].message.content}\n\n"
            return StreamingResponse(
                content=clarification_generator(),
                media_type="text/event-stream"
            )

        chat_request['messages'][-1]['content'] = cleanup_result.choices[0].message.content

        # Extract content from <prompt> tags if present
        # extracted_prompt = extract_prompt_content(prompt)
        # if extracted_prompt is not None:
        #     # Clean the extracted content and update the message
        #     cleaned_prompt = clean_prompt_content(extracted_prompt)
        #     chat_request['messages'][-1]['content'] = cleaned_prompt
        #     print(f"üìù Extracted prompt from tags: {cleaned_prompt}")
        # else:
        #     print(f"üìù Original prompt (no tags): {prompt}")

        result = await openai_client.chat.completions._post(
            "/chat/completions",
            body=chat_request,
            cast_to=ChatCompletion,
            stream=True,
            stream_cls=AsyncStream[ChatCompletionChunk],
        )

        async def event_generator():
            async for chunk in result:
                yield f"data: {chunk.json()}\n\n"
        return StreamingResponse(event_generator(), media_type="text/event-stream")

        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error creating chat completion: {str(e)}"
        )
